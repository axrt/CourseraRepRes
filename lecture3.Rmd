---
title: "Lecture 3"
output: html_document
---


#Distribution and Checklist

##Communicating results

[Getting responces](http://goo.gl/sJDb9V) from busy people

###Article:  
- Title
- Abstract
- Body/Results
- Supplementary Materials/ the gory details
- Code/Data/really gory details

###Email  
- Subject line / Sender info  
    * At minimum include one
    * Can you summarize findings in one sentence
  
- Email body  
    * A breif description of the problem, summarize findings and results
    * If any actions are needed in responce to the presentation, suggest them as concrete as possible
    * If questions need to be answered, make them preferrable yes or no type

###Attachments  
- R Markdown
- `knitr` report
- Stay very concise, no pages of code

###Links to supplementary material
- Code, software, data
- GitHub repo/Project web site


## RPubs

Just registered [here](https://rpubs.com/alext).

##Reproducible research checklist  
**Do**: start with good science  

- Garbage in garbage out
- Coherent, focused question simplifies many problems
- Working with good collaborators reenforce good parctices
- Something that interests motivates good habits

**Don'ts**: do things by hand

- Editing spreadsheets of data to clean them up
    * Removing outliers
    * QA/QC
    * Validating
- Editing tables or figures
- Manual downloading of links from websites
- Moving data around the computer
- "We are just going to do this once" attitude

Anything that is nessesserally done by hand with no other way to automatize should be thoroughly documented.

**More don'ts**: do things by hand

- Do not use point and click soft
- GUI tools are totally irreproducible
- In case there is no way around - try to locate the log if it exists or create one if it does not
- Stuff like text editors are fine

**More dos**: teach a computer how to do stuff for you

- Teaching a computer almost guarantees reproducibility
- Use some version control
    * helps to slow you down
    * add changes in small chunks
    * track, tag etc
    * software like github and bitbucket allow for easy data communication
- Keeping track of the software environment
    * Computer architecture
    * Operating system
    * Software toolchain
    * Support software
    * External dependencies
    * Version numbers
    
```{r session info}
sessionInfo()
```

**More don'ts**: do not save data analysis output (tables, figures, summaries, etc), or export temporary for caching purposes only

- If it is difficult to tell where the files are coming from, the research is not reproducible
- Save the data with the code that produced that data
- Temporary files are ok as long as they are clearly documented

**More dos**: it is improtant to set seeds

- Setting seeds makes the sequences of random numbers reproducible: **always set seed!**
- Important for Monte Carlo and other simulations

```{r set seed}
set.seed(1000)
runif(10)
```

**More dos**: Try to think of the entire pipeline ahead

- Data analysis is a lengthy process
- Raw data -> processed data -> analysis -> report
- How you got to the end is as important as the end itself
- The more of the pipeline you have reproducible, the better off is everyone

#A list of questions to ask yourself:

- Are you doing good science?
- Was any part of the analysis done by hand?
    - If so, were those parts *precisely documented*?
    - Does the documentation match reality?
- Have you taught the computer to do as much as possible?
- Have we been using a version control system?
- Have we saved anything that can't really be tracked back to where it came from?
- Have we documented the environment?
- How far back can we go before the analysis pipeline is no longer reproducible?

Ideally fro me personally that would be going back to the sequencing machine output as i have (and not supposed to have) no control over what had happened befor the sequencing.

#Evidence-based Data Analysis (part 1)

##Replication and Reproducibility

- Focuses ont the validity of the scientific claim
- "Is the claim true?"
- The ultimate standard for strengthening scientific evidence
- New investigators, laboratories, instruments etc
- Particularly important for studies that have broad effect on public opinion and government descisions

###Reproducibility

- Focuses on the validity of the data analysis
- "Can we trust the analysis?"
- Arguably a minimum standard for any scientific research
- New investigators, some methods, some data
- Very improtant when replication is impossible

###Background and underlying trends

- Many studies can not be reproduced: no time, no money, etc
- Due to tech, data is more multidimensional and more complex
- Existing databases can be merge to produce even bigger databases
- Computing power is enough to go for more sophisticated analysis
- For every field "X" there is a "Computational X"

###The result

- Even very basic analyses are difficult to describe
- Heavy computatins are thrown at people with no sufficient statistical training
- Errors are easy to introduce when the piplene analysis is long
- Knowledge transfer is inhibited
- Results are difficult to replicate and reproduce
- Complicated analyses can not be fully trusted

###What problems does reproducibility solve?

- Transparency
- Data Avaliability
- Software/Methods Avaliability
- Improved transfer of knowledge

What we do not get is validity/correctness of the analysis

An analysis can be reproducible, but still be wrong

We want to know whether we can trust this analysis

Does requiring reproducibility defer bad analysis?

[Cochrane Summaries](http://www.cochrane.org/evidence)


